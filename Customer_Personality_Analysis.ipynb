{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bjj7VpGrqQMt"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5714449bd690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P5PJzhxqc5J"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('marketing_campaign.csv',header=0,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9aU9nw7ugDi",
    "outputId": "f90a0eee-aa03-4725-8eaa-6cc6c176a152"
   },
   "outputs": [],
   "source": [
    "!pip install dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WCaRcJmBsPQz",
    "outputId": "ca27faf7-66ad-46db-c8d0-c9761e14e609"
   },
   "outputs": [],
   "source": [
    "from dataprep.eda import plot, plot_correlation, create_report, plot_missing\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "oBrnBXtEsYdP",
    "outputId": "cc0c5721-afd7-47bd-a450-4395bed1729e"
   },
   "outputs": [],
   "source": [
    "#Spending variable creation\n",
    "data['Age']=2014-data['Year_Birth']\n",
    "\n",
    "data['Spending']=data['MntWines']+data['MntFruits']+data['MntMeatProducts']+data['MntFishProducts']+data['MntSweetProducts']+data['MntGoldProds']\n",
    "#Seniority variable creation\n",
    "last_date = date(2014,10, 4)\n",
    "data['Seniority']=pd.to_datetime(data['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')\n",
    "data['Seniority'] = pd.to_numeric(data['Seniority'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')/30\n",
    "data=data.rename(columns={'NumWebPurchases': \"Web\",'NumCatalogPurchases':'Catalog','NumStorePurchases':'Store'})\n",
    "data['Marital_Status']=data['Marital_Status'].replace({'Divorced':'Alone','Single':'Alone','Married':'In couple','Together':'In couple','Absurd':'Alone','Widow':'Alone','YOLO':'Alone'})\n",
    "data['Education']=data['Education'].replace({'Basic':'Undergraduate','2n Cycle':'Undergraduate','Graduation':'Postgraduate','Master':'Postgraduate','PhD':'Postgraduate'})\n",
    "\n",
    "data['Children']=data['Kidhome']+data['Teenhome']\n",
    "data['Has_child'] = np.where(data.Children> 0, 'Has child', 'No child')\n",
    "data['Children'].replace({3: \"3 children\",2:'2 children',1:'1 child',0:\"No child\"},inplace=True)\n",
    "data=data.rename(columns={'MntWines': \"Wines\",'MntFruits':'Fruits','MntMeatProducts':'Meat','MntFishProducts':'Fish','MntSweetProducts':'Sweets','MntGoldProds':'Gold'})\n",
    "\n",
    "\n",
    "data=data[['Age','Education','Marital_Status','Income','Spending','Seniority','Has_child','Children','Wines','Fruits','Meat','Fish','Sweets','Gold']]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCRdR60KsdAW"
   },
   "source": [
    "Now I will remove the outliers and the missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0qNmUW_sd-F"
   },
   "outputs": [],
   "source": [
    "data=data.dropna(subset=['Income'])\n",
    "data=data[data['Income']<600000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RoZa8x8siDY"
   },
   "source": [
    "Clustering\n",
    "In the code section below, I will first normalize the data and then I will create customer clustering according to the metrics defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VAud2i79si-B",
    "outputId": "2084b6e5-5ee2-4b08-fbf5-a67abc696c32"
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "dataset_temp=data[['Income','Seniority','Spending']]\n",
    "X_std=scaler.fit_transform(dataset_temp)\n",
    "X = normalize(X_std,norm='l2')\n",
    "\n",
    "gmm=GaussianMixture(n_components=4, covariance_type='spherical',max_iter=2000, random_state=5).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "dataset_temp['Cluster'] = labels\n",
    "dataset_temp=dataset_temp.replace({0:'Stars',1:'Need attention',2:'High potential',3:'Leaky bucket'})\n",
    "data = data.merge(dataset_temp.Cluster, left_index=True, right_index=True)\n",
    "\n",
    "pd.options.display.float_format = \"{:.0f}\".format\n",
    "summary=data[['Income','Spending','Seniority','Cluster']]\n",
    "summary.set_index(\"Cluster\", inplace = True)\n",
    "summary=summary.groupby('Cluster').describe().transpose()\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBdPfiResuOs"
   },
   "source": [
    "Now letâ€™s plot this data to have a look at the clustering of customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "DxYWIc0csvDO",
    "outputId": "1e99ac99-88f0-459b-ca80-f4a39f00411b"
   },
   "outputs": [],
   "source": [
    "PLOT = go.Figure()\n",
    "for C in list(data.Cluster.unique()):\n",
    "    \n",
    "\n",
    "    PLOT.add_trace(go.Scatter3d(x = data[data.Cluster == C]['Income'],\n",
    "                                y = data[data.Cluster == C]['Seniority'],\n",
    "                                z = data[data.Cluster == C]['Spending'],                        \n",
    "                                mode = 'markers',marker_size = 6, marker_line_width = 1,\n",
    "                                name = str(C)))\n",
    "PLOT.update_traces(hovertemplate='Income: %{x} <br>Seniority: %{y} <br>Spending: %{z}')\n",
    "\n",
    "    \n",
    "PLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n",
    "                   scene = dict(xaxis=dict(title = 'Income', titlefont_color = 'black'),\n",
    "                                yaxis=dict(title = 'Seniority', titlefont_color = 'black'),\n",
    "                                zaxis=dict(title = 'Spending', titlefont_color = 'black')),\n",
    "                   font = dict(family = \"Gilroy\", color  = 'black', size = 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiZ2EQLws3fX"
   },
   "source": [
    "Data Preparation for Customer Personality Analysis\n",
    "Now I will prepare the data for the Apriori algorithm. Here I will be defining three segments of the customers according to the age, income and seniority:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3E4TsnNs0Mb"
   },
   "outputs": [],
   "source": [
    "#Create Age segment\n",
    "cut_labels_Age = ['Young', 'Adult', 'Mature', 'Senior']\n",
    "cut_bins = [0, 30, 45, 65, 120]\n",
    "data['Age_group'] = pd.cut(data['Age'], bins=cut_bins, labels=cut_labels_Age)\n",
    "#Create Income segment\n",
    "cut_labels_Income = ['Low income', 'Low to medium income', 'Medium to high income', 'High income']\n",
    "data['Income_group'] = pd.qcut(data['Income'], q=4, labels=cut_labels_Income)\n",
    "#Create Seniority segment\n",
    "cut_labels_Seniority = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\n",
    "data['Seniority_group'] = pd.qcut(data['Seniority'], q=4, labels=cut_labels_Seniority)\n",
    "data=data.drop(columns=['Age','Income','Seniority'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7eubhFOs-y2"
   },
   "source": [
    "Now I will define new segments according to the spending of customers on each product which will be based on:\n",
    "\n",
    "1. Non Buyer\n",
    "2. Low Buyer\n",
    "3. Frequent Buyer\n",
    "4. Biggest Buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEHecHNds_xf"
   },
   "outputs": [],
   "source": [
    "cut_labels = ['Low consumer', 'Frequent consumer', 'Biggest consumer']\n",
    "data['Wines_segment'] = pd.qcut(data['Wines'][data['Wines']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data['Fruits_segment'] = pd.qcut(data['Fruits'][data['Fruits']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data['Meat_segment'] = pd.qcut(data['Meat'][data['Meat']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data['Fish_segment'] = pd.qcut(data['Fish'][data['Fish']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data['Sweets_segment'] = pd.qcut(data['Sweets'][data['Sweets']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data['Gold_segment'] = pd.qcut(data['Gold'][data['Gold']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
    "data.replace(np.nan, \"Non consumer\",inplace=True)\n",
    "data.drop(columns=['Spending','Wines','Fruits','Meat','Fish','Sweets','Gold'],inplace=True)\n",
    "data = data.astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--_IijgItJSy"
   },
   "source": [
    "Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Yd0T9wQ1tJ_Q",
    "outputId": "a734d052-07a2-48ba-9c55-31f4f8293c26"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "association=data.copy() \n",
    "df = pd.get_dummies(association)\n",
    "min_support = 0.08\n",
    "max_len = 10\n",
    "frequent_items = apriori(df, use_colnames=True, min_support=min_support, max_len=max_len + 1)\n",
    "rules = association_rules(frequent_items, metric='lift', min_threshold=1)\n",
    "\n",
    "product='Wines'\n",
    "segment='Biggest consumer'\n",
    "target = '{\\'%s_segment_%s\\'}' %(product,segment)\n",
    "results_personnal_care = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\n",
    "results_personnal_care.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-P8COIitR-W"
   },
   "source": [
    "Conclusion\n",
    "So according to the output and overall analysis conducted on this data science project on customer personality analysis with Python, we can conclude that the biggest customers of wines are:\n",
    "\n",
    "1. Customers with an average income of around $69,500.\n",
    "2. Customers with an average total spend of approximately $1,252.\n",
    "3. Customers registered with the company for approximately 21 months.\n",
    "4. Customers with a graduate degree.\n",
    "5. And customers who are also heavy consumers of meat products."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Customer Personality Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
